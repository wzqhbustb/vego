# Vego

[![Go Reference](https://pkg.go.dev/badge/github.com/wzqhbustb/vego.svg)](https://pkg.go.dev/github.com/wzqhbustb/vego)
[![Go Report Card](https://goreportcard.com/badge/github.com/wzqhbustb/vego)](https://goreportcard.com/report/github.com/wzqhbustb/vego)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

**Vego** is a lightweight vector search engine for AI agents and embedded applications, written in pure Go with zero CGO dependencies.

```go
// Get started in 5 minutes: create a vector index
index := vego.NewHNSW(vego.Config{
    Dimension: 128,
    M:         16,
})

// Add vectors
id, _ := index.Add([]float32{0.1, 0.2, ...})

// Search nearest neighbors
results, _ := index.Search(query, 10)
```

---

## üéØ Why Vego?

| Feature | Vego | Milvus | Weaviate | FAISS |
|------|------|--------|----------|-------|
| **Deployment** | Embedded Library | Standalone Service | Standalone Service | Python/C++ Lib |
| **Go Native** | ‚úÖ Pure Go | ‚ùå Go+CGO | ‚úÖ Go | ‚ùå C++ |
| **Binary Size** | ~500KB | 100MB+ | 50MB+ | 10MB+ |
| **External Dependencies** | Zero CGO | Many | Many | Python/C++ |
| **Startup Time** | Milliseconds | Seconds | Seconds | - |
| **Persistence** | Built-in | Requires Config | Requires Config | DIY |

### Core Advantages

1. **üöÄ Ultra Lightweight**
   - Pure Go implementation, zero CGO dependencies
   - Single binary deployment
   - Minimal memory footprint (ideal for edge devices)

2. **‚ö° High Performance**
   - HNSW algorithm with millisecond-level latency
   - >95% recall rate (compared to brute-force search)
   - Concurrent read/write support

3. **üíæ Built-in Persistence**
   - Self-developed Lance-like columnar storage format
   - Supports ZSTD, BitPacking, and other compression algorithms
   - One-click save/load complete index

4. **üîß Simple & Easy to Use**
   - Clean API design
   - Supports L2, Cosine, InnerProduct distance metrics
   - Automatic memory management, no complex configuration

---

## üì¶ Use Cases

### 1. AI Agent Local Memory
Provide long-term memory capabilities for AI agents without external databases:

```go
// Agent memory system
memory := vego.NewHNSW(vego.Config{Dimension: 1536})

// Save conversation history
embedding := embed("User loves golang programming")
memory.Add(embedding)

// Retrieve relevant memories
query := embed("What are the user's programming preferences?")
memories, _ := memory.Search(query, 3)
```

### 2. Edge Device Embedded Search
Ideal for IoT, mobile, and edge computing scenarios:
- Local semantic matching for smart home devices
- Anomaly detection for industrial equipment
- Image retrieval for drones

### 3. Microservice Embedded Vector Retrieval
Use directly within services without deploying separate vector databases:
- Candidate set retrieval for recommendation systems
- Document similarity detection
- Log anomaly detection

### 4. RAG (Retrieval-Augmented Generation)
Provide knowledge base retrieval capabilities for local LLMs:
- Local RAG combined with Ollama/Llama.cpp
- Intelligent Q&A for private documents
- Semantic search for code repositories

---

## üöÄ Quick Start

### Installation

```bash
go get github.com/wzqhbustb/vego
```

### Basic Example

```go
package main

import (
    "fmt"
    "github.com/wzqhbustb/vego/index"
)

func main() {
    // 1. Create index
    config := hnsw.Config{
        Dimension:      128,        // Vector dimension
        M:              16,         // Connections (affects recall and memory)
        EfConstruction: 200,        // Build parameter (higher = more accurate)
        DistanceFunc:   hnsw.L2Distance,
    }
    index := hnsw.NewHNSW(config)

    // 2. Add vectors
    vector := make([]float32, 128)
    // ... fill vector data ...
    id, err := index.Add(vector)
    if err != nil {
        panic(err)
    }
    fmt.Printf("Added vector with ID: %d\n", id)

    // 3. Search nearest neighbors
    query := make([]float32, 128)
    // ... fill query vector ...
    results, err := index.Search(query, 10, 0) // Return Top-10
    if err != nil {
        panic(err)
    }

    for _, r := range results {
        fmt.Printf("ID: %d, Distance: %.4f\n", r.ID, r.Distance)
    }
}
```

### Persistence Example

```go
// Save index to disk
err := index.SaveToLance("./my_index")
if err != nil {
    panic(err)
}

// Load index from disk
loadedIndex, err := hnsw.LoadHNSWFromLance("./my_index")
if err != nil {
    panic(err)
}

// Continue using loaded index
results, _ := loadedIndex.Search(query, 10, 0)
```

---

## üìñ API Documentation

### Creating an Index

```go
config := hnsw.Config{
    Dimension:      128,                      // Required: vector dimension
    M:              16,                       // Optional: max connections per layer (default 16)
    EfConstruction: 200,                      // Optional: search scope during build (default 200)
    DistanceFunc:   hnsw.CosineDistance,      // Optional: distance function
    Seed:           42,                       // Optional: random seed
}
index := hnsw.NewHNSW(config)
```

**Distance Function Options:**
- `hnsw.L2Distance` - Euclidean distance (default, for general use)
- `hnsw.CosineDistance` - Cosine distance (for text embeddings)
- `hnsw.InnerProductDistance` - Inner product distance (for semantic search)

### Adding Vectors

```go
id, err := index.Add(vector []float32) (int, error)
```

- Auto-incrementing IDs assigned
- Vectors are deep-copied; modifying original array won't affect index
- Thread-safe, supports concurrent additions

### Searching

```go
results, err := index.Search(
    query []float32,  // Query vector
    k int,            // Number of results to return
    ef int,           // Search scope (0 means use efConstruction)
) ([]SearchResult, error)
```

**Return Value:**
```go
type SearchResult struct {
    ID       int
    Distance float32
}
```

### Parameter Tuning Guide

| Parameter | Small Dataset (<1M) | Large Dataset (>1M) | High Recall | Low Latency |
|-----------|---------------------|---------------------|-------------|-------------|
| M | 16 | 32 | 32-64 | 8-16 |
| EfConstruction | 100-200 | 200-400 | 400+ | 50-100 |
| ef (search) | 50-100 | 100-200 | 200+ | 20-50 |

---

## üèóÔ∏è Architecture

### HNSW Implementation

```
Layer 3: [Entry Point] ---------------------> [Node A]
            ‚Üì                                    ‚Üì
Layer 2: [Entry Point] -----> [Node B] -----> [Node A]
            ‚Üì                     ‚Üì              ‚Üì
Layer 1: [Entry Point] -> [C] -> [B] -> [D] -> [A]
            ‚Üì              ‚Üì      ‚Üì      ‚Üì       ‚Üì
Layer 0: [EP]->[C]->[E]->[F]->[B]->[G]->[D]->[H]->[A]  (Full Graph)
```

- **Multi-layer Graph Structure**: Fast coarse search in upper layers, precise search in lower layers
- **Probabilistic Layer Assignment**: Exponential distribution determines node levels, ensuring O(log N) query efficiency
- **Heuristic Edge Selection**: Considers both distance and neighbor diversity

### Storage Format

Self-developed **Lance-compatible** columnar storage:

```
nodes.lance       # Node data: ID + Vector + Level
connections.lance # Edge data: NodeID + Layer + NeighborID
metadata.lance    # Metadata: M, Dimension, EntryPoint, etc.
```

**Encoding Support:**
- ZSTD compression
- BitPacking integer compression
- RLE run-length encoding
- Dictionary encoding

---

## üìä Performance Benchmarks

Test Environment: Apple M3 Pro, 18GB RAM

### Index Build Performance

| Dataset Size | Dimension | Build Time | Memory Usage |
|-----------|------|----------|----------|
| 10,000 | 128 | 0.12s | 2.1MB |
| 100,000 | 128 | 1.8s | 18MB |
| 1,000,000 | 128 | 25s | 175MB |

### Query Performance (Single-threaded)

| Dataset Size | Top-K | Latency (P99) | Recall |
|-----------|-------|-----------|--------|
| 100,000 | 10 | 0.8ms | 96.5% |
| 100,000 | 100 | 2.1ms | 98.2% |
| 1,000,000 | 10 | 1.5ms | 95.1% |
| 1,000,000 | 100 | 4.2ms | 97.8% |

### Concurrent Performance

```bash
BenchmarkHNSWInsert-12      100000    15230 ns/op    // Concurrent insert
BenchmarkHNSWSearch-12      100000     8920 ns/op    // Concurrent search
```

---

## üîå Integration Examples

### Building Local RAG with Ollama

```go
package main

import (
    "context"
    "github.com/ollama/ollama/api"
    "github.com/wzqhbustb/vego/index"
)

type RAGSystem struct {
    index   *hnsw.HNSWIndex
    client  *api.Client
}

func (r *RAGSystem) AddDocument(ctx context.Context, text string) error {
    // Generate embedding using Ollama
    embedding, err := r.getEmbedding(ctx, text)
    if err != nil {
        return err
    }
    _, err = r.index.Add(embedding)
    return err
}

func (r *RAGSystem) Query(ctx context.Context, question string) (string, error) {
    // 1. Vectorize the question
    queryVec, _ := r.getEmbedding(ctx, question)
    
    // 2. Retrieve relevant documents
    results, _ := r.index.Search(queryVec, 3, 0)
    
    // 3. Construct prompt and query LLM
    context := r.buildContext(results)
    return r.askLLM(ctx, question, context)
}
```

### As AI Agent Memory System

```go
type AgentMemory struct {
    shortTerm []string
    longTerm  *hnsw.HNSWIndex
}

func (m *AgentMemory) Remember(ctx string, embedding []float32) {
    // Long-term memory stored in vector index
    m.longTerm.Add(embedding)
}

func (m *AgentMemory) Recall(query []float32, topK int) []string {
    // Semantic retrieval of relevant memories
    results, _ := m.longTerm.Search(query, topK, 0)
    return m.fetchMemories(results)
}
```

---

## ü§ù Comparison with Other Projects

### vs FAISS
- **FAISS**: Meta's C++ library with Python bindings, powerful but heavy dependencies
- **Vego**: Go-native, no CGO, better suited for Go ecosystem and embedded scenarios

### vs Milvus/Zilliz
- **Milvus**: Enterprise-grade distributed vector database, feature-complete but complex deployment
- **Vego**: Lightweight embedded library, ideal for edge and simple scenarios

### vs Weaviate
- **Weaviate**: Go-implemented vector database requiring standalone deployment
- **Vego**: Pure library form, no service dependencies, faster startup

### vs chroma-go
- **chroma-go**: Go client for Chroma vector database
- **Vego**: No external service dependencies, completely self-contained

---

## üõ†Ô∏è Roadmap

- [x] HNSW core algorithm
- [x] Multiple distance function support
- [x] Persistence storage
- [x] Concurrent safety
- [ ] Vector deletion/update
- [ ] Incremental save
- [ ] Metadata filtering (Filter Search)
- [ ] SIMD acceleration (AVX/NEON)
- [ ] Quantization support (PQ/SQ)
- [ ] Distributed index

---

## ü§≤ Contributing

Contributions are welcome! Whether it's code, bug reports, or suggestions.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Environment

```bash
# Clone repository
git clone https://github.com/wzqhbustb/vego.git
cd vego

# Run tests
go test ./...

# Run benchmarks
go test -bench=. ./index/

# Run recall test
go test -run TestRecall -v ./index/
```

---

## üìÑ License

This project is licensed under the [Apache 2.0 License](LICENSE).

---

## üìö References & Papers

This project is built on top of cutting-edge research in vector search and columnar storage. The following papers have significantly influenced our implementation:

### Small-World Network Theory

| Paper | Authors | Year | Contribution |
|-------|---------|------|--------------|
| **[Collective dynamics of 'small-world' networks](https://doi.org/10.1038/30918)** | Watts & Strogatz | 1998 | Foundational paper on small-world networks; provides the theoretical basis for understanding NSW/HNSW algorithms |
| **[Navigable Networks as Nash Equilibria of Navigation Games](https://doi.org/10.1038/s41467-017-01294-3)** | Papadopoulos et al. | 2018 | Explains why certain network structures exhibit good navigability properties |

### NSW (Navigable Small World) Algorithm

| Paper | Authors | Year | Contribution |
|-------|---------|------|--------------|
| **[Approximate nearest neighbor algorithm based on navigable small world graphs](https://doi.org/10.1016/j.ins.2013.08.017)** | Malkov et al. | 2014 | The predecessor to HNSW; introduces approximate nearest neighbor search based on navigable small-world graphs |
| **[Scalable Distributed Algorithm for Approximate Nearest Neighbor Search Problem in High Dimensional General Metric Spaces](https://doi.org/10.1109/TPDS.2015.2505333)** | Malkov et al. | 2016 | Early improvements to NSW, focusing on scalability and distributed processing |

### HNSW (Hierarchical NSW) Algorithm

| Paper | Authors | Year | Contribution |
|-------|---------|------|--------------|
| **[Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs](https://arxiv.org/abs/1603.09320)** | Malkov & Yashunin | 2016 | **Core algorithm** used by Vego; extends NSW with hierarchical structure for O(log N) query complexity |
| **[A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate Nearest Neighbor Search](https://doi.org/10.14778/3397230)** | Li et al. | 2020 | Comparative analysis of various graph-based ANN algorithms including HNSW; guided our implementation decisions |

### Large-Scale Vector Search

| Paper | Authors | Year | Contribution |
|-------|---------|------|--------------|
| **[Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)** | Johnson et al. | 2017 | Facebook AI's FAISS library; provides comprehensive comparison of methods including HNSW |
| **[DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node](https://doi.org/10.14778/3424573)** | Subramanya et al. | 2019 | Microsoft's improved approach for billion-scale data on SSDs; informs our future roadmap for large-scale support |

### Columnar Storage & Vector Quantization

| Paper | Authors | Year | Contribution |
|-------|---------|------|--------------|
| **[Apache Arrow: Cross-Language Development Platform for In-Memory Analytics](https://doi.org/10.14778/3397230)** | Apache Arrow Team | 2016 | Foundation for our storage layer's in-memory representation |
| **[Lance: Efficient Random Access in Columnar Storage through Adaptive Structural Encodings](https://github.com/lancedb/lance)** | Lance Team | 2022 | Influenced our Lance-compatible columnar storage format design |
| **[Lance v2: A New Columnar Container Format](https://lancedb.github.io/lance/format.html)** | Lance Team | 2023 | Latest columnar container format improvements |
| **[Product Quantization for Nearest Neighbor Search](https://doi.org/10.1109/TPAMI.2010.57)** | J√©gou et al. | 2011 | Foundation for future quantization support (PQ) to reduce memory footprint |

---

## üôè Acknowledgments

- **HNSW Algorithm**: Yury A. Malkov and Dmitry A. Yashunin for the groundbreaking HNSW algorithm
- **Apache Arrow Project**: For the standardized columnar memory format
- **Lance**: For the modern columnar storage format designed for ML/AI workloads
- **Go Community**: For the excellent ecosystem and tools

---

## üìÆ Contact

- Project Homepage: https://github.com/wzqhbustb/vego
- Issue Tracker: [GitHub Issues](https://github.com/wzqhbustb/vego/issues)
- Discussions: [GitHub Discussions](https://github.com/wzqhbustb/vego/discussions)

---

<p align="center">
  <b>If you find this project helpful, please give it a ‚≠êÔ∏è Star!</b>
</p>
